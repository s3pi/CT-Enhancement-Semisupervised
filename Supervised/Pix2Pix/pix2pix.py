# -*- coding: utf-8 -*-
"""cyclegan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb

##### Copyright 2019 The TensorFlow Authors.
"""

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""# CycleGAN

<table class="tfo-notebook-buttons" align="left">
  <td>
    <a target="_blank" href="https://www.tensorflow.org/tutorials/generative/cyclegan"><img src="https://www.tensorflow.org/images/tf_logo_32px.png" />View on TensorFlow.org</a>
  </td>
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb"><img src="https://www.tensorflow.org/images/colab_logo_32px.png" />Run in Google Colab</a>
  </td>
  <td>
    <a target="_blank" href="https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb"><img src="https://www.tensorflow.org/images/GitHub-Mark-32px.png" />View source on GitHub</a>
  </td>
  <td>
    <a href="https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/generative/cyclegan.ipynb"><img src="https://www.tensorflow.org/images/download_logo_32px.png" />Download notebook</a>
  </td>
</table>

This notebook demonstrates unpaired image to image translation using conditional GAN's, as described in [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593), also known as CycleGAN. The paper proposes a method that can capture the characteristics of one image domain and figure out how these characteristics could be translated into another image domain, all in the absence of any paired training examples. 

This notebook assumes you are familiar with Pix2Pix, which you can learn about in the [Pix2Pix tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix). The code for CycleGAN is similar, the main difference is an additional loss function, and the use of unpaired training data.

CycleGAN uses a cycle consistency loss to enable training without the need for paired data. In other words, it can translate from one domain to another without a one-to-one mapping between the source and target domain. 

This opens up the possibility to do a lot of interesting tasks like photo-enhancement, image colorization, style transfer, etc. All you need is the source and the target dataset (which is simply a directory of images).

![Output Image 1](images/horse2zebra_1.png)
![Output Image 2](images/horse2zebra_2.png)

## Set up the input pipeline

Install the [tensorflow_examples](https://github.com/tensorflow/examples) package that enables importing of the generator and the discriminator.
"""

# !pip install git+https://github.com/tensorflow/examples.git

import tensorflow as tf

# import tensorflow_datasets as tfds
import sys
sys.path.insert(1, '/home/ada/Preethi/PGI_CT_Scan/Unsupervised/Code/CycleGAN')
from examples_master.tensorflow_examples.models.pix2pix import pix2pix

import os
import time
import matplotlib.pyplot as plt
import numpy as np
from sklearn.utils import shuffle
import math
from skimage.measure import compare_ssim as ssim
import cv2
from statistics import mean
# tf.enable_eager_execution()
import time
import random
# from IPython.display import clear_output

# tfds.disable_progress_bar()
# AUTOTUNE = tf.data.experimental.AUTOTUNE

"""## Input Pipeline

This tutorial trains a model to translate from images of horses, to images of zebras. You can find this dataset and similar ones [here](https://www.tensorflow.org/datasets/datasets#cycle_gan). 

As mentioned in the [paper](https://arxiv.org/abs/1703.10593), apply random jittering and mirroring to the training dataset. These are some of the image augmentation techniques that avoids overfitting.

This is similar to what was done in [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix#load_the_dataset)

* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`.
* In random mirroring, the image is randomly flipped horizontally i.e left to right.
"""

# dataset, metadata = tfds.load('cycle_gan/horse2zebra',
#                               with_info=True, as_supervised=True)

# train_horses, train_zebras = dataset['trainA'], dataset['trainB']
# test_horses, test_zebras = dataset['testA'], dataset['testB']

# def random_crop(image):
#   cropped_image = tf.image.random_crop(
#       image, size=[IMG_HEIGHT, IMG_WIDTH, 3])

#   return cropped_image

# normalizing the images to [-1, 1]
def normalize(image):
  image = tf.cast(image, tf.float32)
  image = (image / 127.5) - 1
  return image

def min_max_normalization(img, img_min, img_max):
    img = (img - img_min) / (img_max - img_min)
    return img

# def random_jitter(image):
#   # resizing to 286 x 286 x 3
#   image = tf.image.resize(image, [286, 286],
#                           method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)

#   # randomly cropping to 256 x 256 x 3
#   image = random_crop(image)

#   # random mirroring
#   image = tf.image.random_flip_left_right(image)

#   return image

# def preprocess_image_train(image, label):
#   image = random_jitter(image)
#   image = normalize(image)
#   return image

# def preprocess_image_test(image, label):
#   image = normalize(image)
#   return image

# train_horses = train_horses.map(
#     preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(
#     BUFFER_SIZE).batch(1)

# train_zebras = train_zebras.map(
#     preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(
#     BUFFER_SIZE).batch(1)

# test_horses = test_horses.map(
#     preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(
#     BUFFER_SIZE).batch(1)

# test_zebras = test_zebras.map(
#     preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(
#     BUFFER_SIZE).batch(1)

# sample_horse = next(iter(train_horses))
# sample_zebra = next(iter(train_zebras))

# plt.subplot(121)
# plt.title('Horse')
# plt.imshow(sample_horse[0] * 0.5 + 0.5)

# plt.subplot(122)
# plt.title('Horse with random jitter')
# plt.imshow(random_jitter(sample_horse[0]) * 0.5 + 0.5)

# plt.subplot(121)
# plt.title('Zebra')
# plt.imshow(sample_zebra[0] * 0.5 + 0.5)

# plt.subplot(122)
# plt.title('Zebra with random jitter')
# plt.imshow(random_jitter(sample_zebra[0]) * 0.5 + 0.5)

"""## Import and reuse the Pix2Pix models

Import the generator and the discriminator used in [Pix2Pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) via the installed [tensorflow_examples](https://github.com/tensorflow/examples) package.

The model architecture used in this tutorial is very similar to what was used in [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py). Some of the differences are:

* Cyclegan uses [instance normalization](https://arxiv.org/abs/1607.08022) instead of [batch normalization](https://arxiv.org/abs/1502.03167).
* The [CycleGAN paper](https://arxiv.org/abs/1703.10593) uses a modified `resnet` based generator. This tutorial is using a modified `unet` generator for simplicity.

There are 2 generators (G and F) and 2 discriminators (X and Y) being trained here. 

* Generator `G` learns to transform image `X` to image `Y`. $(G: X -> Y)$
* Generator `F` learns to transform image `Y` to image `X`. $(F: Y -> X)$
* Discriminator `D_X` learns to differentiate between image `X` and generated image `X` (`F(Y)`).
* Discriminator `D_Y` learns to differentiate between image `Y` and generated image `Y` (`G(X)`).

![Cyclegan model](images/cyclegan_model.png)
"""
def make_model():
  generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')
  generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')

  discriminator_x = pix2pix.discriminator(OUTPUT_CHANNELS, norm_type='instancenorm', target=False)
  discriminator_y = pix2pix.discriminator(OUTPUT_CHANNELS, norm_type='instancenorm', target=False)

  return generator_g, generator_f, discriminator_x, discriminator_y

# to_zebra = generator_g(sample_horse)
# to_horse = generator_f(sample_zebra)
# plt.figure(figsize=(8, 8))
# contrast = 8

# imgs = [sample_horse, to_zebra, sample_zebra, to_horse]
# title = ['Horse', 'To Zebra', 'Zebra', 'To Horse']

# for i in range(len(imgs)):
#   plt.subplot(2, 2, i+1)
#   plt.title(title[i])
#   if i % 2 == 0:
#     plt.imshow(imgs[i][0] * 0.5 + 0.5)
#   else:
#     plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)
# plt.show()

# plt.figure(figsize=(8, 8))

# plt.subplot(121)
# plt.title('Is a real zebra?')
# plt.imshow(discriminator_y(sample_zebra)[0, ..., -1], cmap='RdBu_r')

# plt.subplot(122)
# plt.title('Is a real horse?')
# plt.imshow(discriminator_x(sample_horse)[0, ..., -1], cmap='RdBu_r')

# plt.show()

"""## Loss functions

In CycleGAN, there is no paired data to train on, hence there is no guarantee that the input `x` and the target `y` pair are meaningful during training. Thus in order to enforce that the network learns the correct mapping, the authors propose the cycle consistency loss.

The discriminator loss and the generator loss are similar to the ones used in [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix#define_the_loss_functions_and_the_optimizer).
"""


"""Cycle consistency means the result should be close to the original input. For example, if one translates a sentence from English to French, and then translates it back from French to English, then the resulting sentence should be the same as the  original sentence.

In cycle consistency loss, 

* Image $X$ is passed via generator $G$ that yields generated image $\hat{Y}$.
* Generated image $\hat{Y}$ is passed via generator $F$ that yields cycled image $\hat{X}$.
* Mean absolute error is calculated between $X$ and $\hat{X}$.

$$forward\ cycle\ consistency\ loss: X -> G(X) -> F(G(X)) \sim \hat{X}$$

$$backward\ cycle\ consistency\ loss: Y -> F(Y) -> G(F(Y)) \sim \hat{Y}$$


![Cycle loss](images/cycle_loss.png)
"""

def calc_cycle_loss(real_image, cycled_image):
  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))
  
  return LAMBDA * loss1

"""As shown above, generator $G$ is responsible for translating image $X$ to image $Y$. Identity loss says that, if you fed image $Y$ to generator $G$, it should yield the real image $Y$ or something close to image $Y$.

$$Identity\ loss = |G(Y) - Y| + |F(X) - X|$$
"""
"""## Training

Note: This example model is trained for fewer epochs (40) than the paper (200) to keep training time reasonable for this tutorial. Predictions may be less accurate.
"""

def generate_images(model, test_input):
  prediction = model(test_input)
  return prediction
    
  # plt.figure(figsize=(12, 12))

  # display_list = [test_input[0], prediction[0]]
  # title = ['Input Image', 'Predicted Image']

  # for i in range(2):
  #   plt.subplot(1, 2, i+1)
  #   plt.title(title[i])
  #   # getting the pixel values between [0, 1] to plot it.
  #   plt.imshow(display_list[i] * 0.5 + 0.5)
  #   plt.axis('off')
  # plt.show()

"""Even though the training loop looks complicated, it consists of four basic steps:

* Get the predictions.
* Calculate the loss.
* Calculate the gradients using backpropagation.
* Apply the gradients to the optimizer.
"""


def discriminator_loss(disc_real_output, disc_generated_output):
  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)

  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)

  total_disc_loss = real_loss + generated_loss

  return total_disc_loss

def generator_loss(disc_generated_output, gen_output, target):
  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)

  # mean absolute error
  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))

  total_gen_loss = gan_loss + (LAMBDA * l1_loss)

  return total_gen_loss, gan_loss, l1_loss


def train_step(input_image, target):
  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
    gen_output = generator(input_image, training=True)

    disc_real_output = discriminator([input_image, target], training=True)
    disc_generated_output = discriminator([input_image, gen_output], training=True)

    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)
    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)

  generator_gradients = gen_tape.gradient(gen_total_loss,
                                          generator.trainable_variables)
  discriminator_gradients = disc_tape.gradient(disc_loss,
                                               discriminator.trainable_variables)

  generator_optimizer.apply_gradients(zip(generator_gradients,
                                          generator.trainable_variables))
  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,
                                              discriminator.trainable_variables))
  return gen_total_loss, disc_loss

def make_path_matrices(root_folder): 
    img_mat = []
    all_subs = sorted(os.listdir(root_folder))
    for each_sub in all_subs:
        each_sub_imgs = []
        all_slices_path = os.path.join(root_folder, each_sub)
        all_slices = sorted(os.listdir(all_slices_path))
        for each_slice in all_slices:
            each_slice_path = os.path.join(all_slices_path, each_slice)
            each_sub_imgs.append(each_slice_path)

        each_sub_imgs = np.asarray(each_sub_imgs) #is it necessary to make it numpy array: easy to find shape
        img_mat.append(each_sub_imgs)
        # print(each_sub_imgs.shape)

    img_mat = np.asarray(img_mat) #is it necessary to make it numpy array
    # print(img_mat.shape)

    return img_mat

def name_numbers(length, number):
    return '0' * (length - len(str(number))) + str(number)

  
def make_directories(val_subs):
  for each in ['Result_Files', 'Images', 'best_npy_files', 'Model_Weights']:
      file_path = os.path.join(local_path, each)
      try:
          os.mkdir(file_path)
      except (FileExistsError):
          pass      
      if each != 'Model_Weights':
          for each_val_sub in val_subs:
              each_val_sub_num = name_numbers(3, each_val_sub)
              val_sub_path = os.path.join(file_path, each_val_sub_num)
              try:
                  os.mkdir(val_sub_path)
              except (FileExistsError):
                  pass

def write_metric_files(files, values):
  for i in range(len(files)):
      files[i].write(str(values[i])+ '\n')

def make_train_path_matrices():
    train_data_path_mat = []
    train_label_path_mat = []

    random.shuffle(train_subs)
    for each_sub in train_subs:     
        train_data_path_mat.extend(KVP_70_img_path_mat[each_sub])
        train_label_path_mat.extend(KVP_100_img_path_mat[each_sub])

    train_data_path_mat = np.asarray(train_data_path_mat)
    train_label_path_mat = np.asarray(train_label_path_mat)

    return train_data_path_mat, train_label_path_mat, train_subs

def find_min_max_of_img(number, train_subs):
    cumm_sum = 0
    for each_sub in train_subs:
        cumm_sum += num_of_slices_per_sub[each_sub]
        if number <= cumm_sum:
            return each_sub
    else:
        print(number)
        print(cumm_sum)
        print(train_subs)
        print(num_of_slices_per_sub)
        print("check")
        exit()

def make_model():
  generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')
  # generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')

  discriminator_x = pix2pix.discriminator(OUTPUT_CHANNELS, norm_type='instancenorm', target=False)
  # discriminator_y = pix2pix.discriminator(OUTPUT_CHANNELS, norm_type='instancenorm', target=False)

  return generator_g, discriminator_x


def make_valid_path_matrices():
    valid_data_path_mat = []
    valid_label_path_mat = []
    
    for each_valid_sub in valid_subs:
        valid_data_path_mat.append(KVP_70_img_path_mat[each_valid_sub])
        valid_label_path_mat.append(KVP_100_img_path_mat[each_valid_sub])

    valid_data_path_mat = np.asarray(valid_data_path_mat)
    valid_label_path_mat = np.asarray(valid_label_path_mat)

    return valid_data_path_mat, valid_label_path_mat

def validate(epoch):
  global max_psnr
  global save_model_list
  mse_all_val_subs = []
  psnr_all_val_subs = []
  mae_all_val_subs = []
  ssim_all_val_subs = [] 
  pred_all_val_subs = []
  
  best_npy_files_path = os.path.join(local_path, 'best_npy_files')

  val_data_path_mat, val_label_path_mat = make_valid_path_matrices()

  avg_psnr_file = open(Result_Files_path + '/avg_psnr' + '.txt', 'a')
  avg_mse_file = open(Result_Files_path+ '/avg_mse' + '.txt', 'a')
  avg_mae_file = open(Result_Files_path+ '/avg_mae' + '.txt', 'a')
  avg_ssim_file = open(Result_Files_path + '/avg_ssim.txt', 'a')

  for i in range(len(valid_subs)):
    val_sub_name = name_numbers(3, valid_subs[i])
    val_sub_path = os.path.join(Result_Files_path, str(val_sub_name))

    psnr_file = open(val_sub_path+ '/psnr' + '.txt', 'a')
    mse_file = open(val_sub_path+ '/mse' + '.txt', 'a')
    mae_file = open(val_sub_path+ '/mae' + '.txt', 'a')
    ssim_file = open(val_sub_path + '/ssim.txt', 'a')

    val_data = np.zeros((len(val_label_path_mat[i]), 512, 512, 1))
    prediction = np.zeros((len(val_label_path_mat[i]), 512, 512, 1))
    val_label = np.zeros((len(val_label_path_mat[i]), 512, 512, 1))

    min_data_val = min_max_of_subs_KVP_70[valid_subs[i]][0]
    max_data_val = min_max_of_subs_KVP_70[valid_subs[i]][1]
    min_label_val = min_max_of_subs_KVP_100[valid_subs[i]][0]
    max_label_val = min_max_of_subs_KVP_100[valid_subs[i]][1]
    
    for k in range(len(val_label_path_mat[i])):  
      data = np.load(val_data_path_mat[i][k])
      val_data_slice = min_max_normalization(data, min_data_val, max_data_val)
      val_data_slice = np.expand_dims(val_data_slice, 0)
      val_data_slice = val_data_slice[:,:,:,np.newaxis]
      val_data[k,:,:,:] = val_data_slice

      prediction[k,:,:,:] = generate_images(generator, val_data_slice)

      label = np.load(val_label_path_mat[i][k])
      val_label_slice = min_max_normalization(label, min_label_val, max_label_val)
      val_label_slice = np.expand_dims(val_label_slice, 0)
      val_label[k,:,:,:] = val_label_slice[:,:,:,np.newaxis]

    pred_all_val_subs.append(prediction[:,:,:,0])

    mse_all_val_subs.append(math.sqrt(np.mean((val_label[:,:,:,0] - prediction[:,:,:,0]) ** 2)))
    psnr_all_val_subs.append(20 * math.log10( 1.0 / (mse_all_val_subs[-1])))
    mae_all_val_subs.append(np.mean(np.abs((val_label[:,:,:,0] - prediction[:,:,:,0]))))
    ssim_all_val_subs.append(ssim(prediction[:,:,:,0], val_label[:,:,:,0], multichannel=True))

    psnr_file.write("%f \n" %(psnr_all_val_subs[-1]))
    mse_file.write("%f \n" %(mse_all_val_subs[-1]))
    mae_file.write("%f \n" %(mae_all_val_subs[-1]))
    ssim_file.write("%f \n" %(ssim_all_val_subs[-1]))

    if (epoch % save_at_every == 0) or (epoch == (EPOCHS - 1)):
      for slice_num in [20,80,100,120,140]:
        temp = np.zeros([img_size, img_size*4])
        temp[:img_size,:img_size] = val_data[slice_num,:,:,0]
        temp[:img_size,img_size:img_size*2] = val_label[slice_num,:,:,0]
        temp[:img_size,img_size*2:img_size*3] = prediction[slice_num,:,:,0]
        temp[:img_size,img_size*3:] = abs(prediction[slice_num,:,:,0] - val_label[slice_num,:,:,0])
        temp = temp * 255
        path = os.path.join(Images_path, str(val_sub_name))
        cv2.imwrite(path + "/EpochNum"+ str(epoch) + "_Slice_num" + str(slice_num) + '.jpg', temp)

    psnr_file.close()
    mse_file.close()
    mae_file.close()
    ssim_file.close()

  avg_psnr = mean(psnr_all_val_subs)
  avg_psnr_file.write("%f \n" %avg_psnr)
  avg_mse_file.write("%f \n" %(mean(mse_all_val_subs)))
  avg_mae_file.write("%f \n" %(mean(mae_all_val_subs)))
  avg_ssim_file.write("%f \n" %(mean(ssim_all_val_subs)))

  if avg_psnr > max_psnr:
    save_model_list.append(epoch)
    generator.save_weights(Model_Weights_path + '/generator' + str(epoch)+'.h5')
    discriminator.save_weights(Model_Weights_path + '/discriminator' + str(epoch)+'.h5')
    i = 0
    for p in valid_subs:
      name = name_numbers(3, p)
      path = os.path.join(best_npy_files_path, str(name))
      np.save(path + "/EpochNum"+ str(epoch) + ".npy", pred_all_val_subs[i][:,:,:])
      i += 1

    if len(save_model_list) > max_saved_models:
        del_model_count = save_model_list.pop(0)
        os.remove(Model_Weights_path + '/generator' + str(del_model_count)+'.h5')
        os.remove(Model_Weights_path + '/discriminator' + str(del_model_count)+'.h5')
        for p in val_subs:
          name = name_numbers(3, p)
          path = os.path.join(best_npy_files_path, str(name))
          os.remove(path + "/EpochNum"+ str(del_model_count) + ".npy")
      
    print('Model loss improved from %f to %f\n'%(max_psnr, avg_psnr))
    max_psnr = avg_psnr

  else:
    print('Best psnr is: %f\n'%(max_psnr))

  avg_psnr_file.close()
  avg_mse_file.close()
  avg_mae_file.close()
  avg_ssim_file.close()

def train():
    start_time = time.time()
    valid_data_path_mat, valid_label_path_mat = make_valid_path_matrices()
    
    make_directories(valid_subs)

    max_psnr = 0

    for jj in range(EPOCHS):
        print('time in secs', time.time() - start_time)
        start_time = time.time()
        # with open(log_file_path, 'a') as log_file:
        #     log_file.write("Running epoch : %d" % jj)

        # Shuffling patient wise each epoch.
        train_data_path_mat, train_label_path_mat, train_subs = make_train_path_matrices()

        # Creating text file to store training loss metrics.
        batch_loss_file = open(Result_Files_path + '/batch_loss_file' + '.txt', 'a')
        batch_loss_per_epoch_file = open(Result_Files_path + '/batch_loss_per_epoch' + '.txt', 'a')
    
        batch_loss_per_epoch = 0.0
        num_batches = int(len(train_data_path_mat)/batch_size)

        for batch in range(num_batches):
        # for batch in range(1):
            batch_train_data = np.zeros((batch_size, img_size, img_size, 1))
            batch_train_label = np.zeros((batch_size, img_size, img_size, 1))
            element_in_batch = 0

            for each_npy in range(batch*batch_size, min((batch+1) * batch_size, len(train_data_path_mat))):
                sub_num = find_min_max_of_img((batch - 1)*batch_size + each_npy + 1, train_subs)
                min_data_val = min_max_of_subs_KVP_70[sub_num][0]
                max_data_val = min_max_of_subs_KVP_70[sub_num][1]
                min_label_val = min_max_of_subs_KVP_100[sub_num][0]
                max_label_val = min_max_of_subs_KVP_100[sub_num][1]

                batch_train_data[element_in_batch, :, :, 0] = min_max_normalization(np.load(train_data_path_mat[each_npy]), min_data_val, max_data_val)
                batch_train_label[element_in_batch, :, :, 0] = min_max_normalization(np.load(train_label_path_mat[each_npy]), min_label_val, max_label_val)

                element_in_batch += 1
                
            gen_total_loss, disc_loss = train_step(batch_train_data, batch_train_label)
            write_metric_files([batch_loss_file], [[gen_total_loss, disc_loss]])

        validate(jj)
            # with open(log_file_path, 'a') as log_file:
            #     log_file.write('epoch_num: %d batch_num: %d loss: %f\n' % (jj, batch, loss))
        
        mse_img = []
        psnr_img = []
        mae_img = []
        ssim_val = []
        valid_subs_decoded_imgs = []

    batch_loss_file.close()
    batch_loss_per_epoch_file.close()

def find_min_max_of_subs():
    min_max_of_subs_KVP_70 = {}
    min_max_of_subs_KVP_100 = {}
    num_of_slices_per_sub = {}

    for i in range(num_of_subs):
        sub_KVP_70 = np.zeros((len(KVP_70_img_path_mat[i]), 512, 512))
        for k in range(len(KVP_70_img_path_mat[i])):
            sub_KVP_70[k] = np.load(KVP_70_img_path_mat[i][k])

        sub_KVP_100 = np.zeros((len(KVP_100_img_path_mat[i]), 512, 512))
        for k in range(len(KVP_100_img_path_mat[i])):
            sub_KVP_100[k] = np.load(KVP_100_img_path_mat[i][k])

        min_max_of_subs_KVP_70[i] = [np.min(sub_KVP_70), np.max(sub_KVP_70)]
        min_max_of_subs_KVP_100[i] = [np.min(sub_KVP_100), np.max(sub_KVP_100)]
        num_of_slices_per_sub[i] = len(KVP_70_img_path_mat[i])

    return (min_max_of_subs_KVP_70, min_max_of_subs_KVP_100, num_of_slices_per_sub)

#--------------------------------------------------------------------------------------------------------
# Arguments:
pro_data_path = "/home/ada/Preethi/PGI_CT_Scan/Paired_Data/Data/processed_data/un_norm"
# BUFFER_SIZE = 1000
img_size = 512
valid_subs = [2, 3]
train_subs = list(set(range(8))-set(valid_subs))
max_psnr = 0
num_of_subs = 8
#--------------------------------------------------------------------------------------------------------
local_path = "/home/ada/Preethi/PGI_CT_Scan/Supervised/Code/Pix2Pix/Results"
Result_Files_path = os.path.join(local_path, 'Result_Files')
Model_Weights_path = os.path.join(local_path, 'Model_Weights')
Images_path = os.path.join(local_path, 'Images')
best_npy_files_path = os.path.join(local_path, 'best_npy_files')
#--------------------------------------------------------------------------------------------------------
batch_size = 2
EPOCHS = 1000
save_at_every = 2
max_saved_models = 4
save_model_list = []
#--------------------------------------------------------------------------------------------------------
KVP_70 = pro_data_path + "/KVP_70"
KVP_70_img_path_mat = make_path_matrices(KVP_70)

KVP_100 = pro_data_path + "/KVP_100"
KVP_100_img_path_mat = make_path_matrices(KVP_100)

min_max_of_subs_KVP_70, min_max_of_subs_KVP_100, num_of_slices_per_sub = find_min_max_of_subs()
#--------------------------------------------------------------------------------------------------------
OUTPUT_CHANNELS = 1
LAMBDA = 10
loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)
generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
generator, discriminator = make_model()
#--------------------------------------------------------------------------------------------------------
start_time = time.time()
train()


"""## Next steps

This tutorial has shown how to implement CycleGAN starting from the generator and discriminator implemented in the [Pix2Pix](https://www.tensorflow.org/tutorials/generative/pix2pix) tutorial. As a next step, you could try using a different dataset from [TensorFlow Datasets](https://www.tensorflow.org/datasets/datasets#cycle_gan). 

You could also train for a larger number of epochs to improve the results, or you could implement the modified ResNet generator used in the [paper](https://arxiv.org/abs/1703.10593) instead of the U-Net generator used here.
instead of the U-Net generator used here. 
/pix2pix) tutorial. As a next step, you could try using a different dataset from [TensorFlow Datasets](https://www.tensorflow.org/datasets/datasets#cycle_gan). 

You could also train for a larger number of epochs to improve the results, or you could implement the modified ResNet generator used in the [paper](https://arxiv.org/abs/1703.10593) instead of the U-Net generator used here.
instead of the U-Net generator used here. /arxiv.org/abs/1703.10593) instead of the U-Net generator used here.
instead of the U-Net generator used here. 
"""